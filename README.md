## ğŸ“š Overview

**LLMâ€¯\_Reasoning** is a *minimal, endâ€‘toâ€‘end recipe* for giving largeâ€‘languageâ€‘models stronger mathematicalâ€‘reasoning skills.
It shows how to

1. **Reâ€‘annotate raw math datasets** into Chainâ€‘ofâ€‘Thoughtâ€‘style (CoT) records (ğŸ—‚Â `data_process/`)â€¯([github.com][1])
2. **Fineâ€‘tune a base model** with supervised CoT (ğŸ—‚Â `train/`)â€¯([github.com][2])
3. **Train a reward model** and **run rejection sampling** to automatically filter lowâ€‘quality traces (ğŸ—‚Â `reward_model/`,Â `reject_sampling/`)â€¯([github.com][3])

The repo is intentionally lightweightâ€”only Pythonâ€¯+â€¯Shell scriptsâ€”so you can slot it into any larger alignment workflow.

---

## ğŸ—ºï¸ Directory structure


| Path               | Purpose                                                                                                                                           |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| `data_process`    | Convert OpenR1â€‘MathÂ 220k parquet shards into Alpacaâ€‘style JSON/Parquet with CoT answersÂ (`data_handler.py`)[./data_process/data_handler.py]   |
| `train_sft`           | Supervisedâ€‘fineâ€‘tuning (SFT) scripts. Example bash launcher in`train.sh`Â shows the core HFÂ `Trainer` flagsÂ (fp16/bf16, long context, etc.)â€¯ |
| `reward_model`    | Rewardâ€‘model training (pairwise ranking / Bradleyâ€‘Terryâ€‘style loss).                                                                           |
| `reject_sampling/` | Sample N answers per prompt, score with the reward model, keep the best.                                                                          |
| `*.sh`             | Oneâ€‘liner helpers (`data_preprocess.sh`, `test_data_preprocess.sh`) for quick sanity tests                |
| `requirements.txt` | Fully pinned dependency list for reproducibility             |

---

## ğŸ”§ Installation

```bash
# 1. Clone
git clone https://github.com/ShenJiarun/LLM_Reasoning.git
cd LLM_Reasoning

# 2. (Recommended) create a fresh environment
python -m venv .venv && source .venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt
```

Key libraries and why they matter

* **ğŸ¤—â€¯TransformersÂ 4.52** â€“ longâ€‘context support and speculation decoding hooks
* **PEFTÂ 0.7.1** â€“ LoRA / QLoRA for memoryâ€‘efficient SFT
* **Hydraâ€‘CoreÂ 1.3** â€“ clean experiment config management
* **TRL** â€“ readyâ€‘made rewardâ€‘model and RLHF trainers
* **vLLM** - effective inference backend

---

## ğŸš€ Quick start

### 1.Â Prepare data

Plesase first download [open-r1/OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k/tree/472472d80032b525579a78b5fb8cf5c548ccccd4/extended) dataset from HuggingFace ğŸ¤—

```bash
bash data_preprocess.sh
# âœ  outputs reason_cot_data.parquet with columns: instruction | input | output
```

You can get your first reasoning dataset!

### 2.Â Supervised fineâ€‘tune

```bash
bash train.sh
# default: 32â€‘bit AdamW â†’ bf16, 1eâ€‘5 lr, max_len 327â€¯k tokens
```

After training is done, please check the result model in the output_dir path ğŸ¤—

### 3.Â Reward model + rejection sampling

After SFT finishes:

check `reject_sampling/sample.py` for further inferencing!

---

## ğŸ§ª Evaluation

The repo does not hardâ€‘code a benchmark to stay frameworkâ€‘agnostic.
*Suggested*: run **MATHÂ datasets exactâ€‘answer accuracy** or **BIGâ€‘Bench\_hard** on the filtered outputs, and compare to the base model.

---

## ğŸ¤ Contributing

PRs are welcome! If you add new datasets or plug in DeepSpeed/FSâ€‘DP, please:

1. Follow the existing logging pattern (Hydra +Â `accelerate`).
2. Update `requirements.txt` only with minimal, versionâ€‘pinned additions.
3. Document new CLI flags in code comments and this README.

---

## ğŸ“œ License

Currently no license file is presentâ€”so the default is â€œall rights reserved.â€
Open an issue if you need explicit MIT/Apache licensing.

---

## âœ¨ Acknowledgements

* OpenR1â€‘MathÂ 220k dataset for raw problems
* Hugging Face ecosystem (Transformers, PEFT, TRL)
* Everyone sharing ideas on structured reasoning data and reward modeling

---

<div style="text-align: center;">
  Happy reasoning! ğŸ§ â•ğŸ¤–
</div>
